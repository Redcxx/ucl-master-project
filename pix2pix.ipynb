{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pix2pix.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMBZKlopLWG8SWXyXGPYwWj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcxx/ucl-master-project/blob/master/pix2pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Settings"
      ],
      "metadata": {
        "id": "247rtPpgAHwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment"
      ],
      "metadata": {
        "id": "DjZs_l1Pz0Ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pydrive2 > /dev/null\n",
        "%pip install torchinfo > /dev/null"
      ],
      "metadata": {
        "id": "b6AcNCCGK8nn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import functools\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "id": "2kQXwgfm5sVP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "ULAvvWgOvvuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SessionConfig(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "\n",
        "        # House keeping\n",
        "        self.run_id = datetime.now().strftime('%Y-%m-%d-%A-%Hh-%Mm-%Ss')\n",
        "        self.random_seed = 42\n",
        "        self.working_folder = 'MasterProject'  # will be created on google drive at root\n",
        "        self.pydrive2_setting_file = 'settings.yaml'\n",
        "\n",
        "        # Dataset\n",
        "        self.batch_size = 1\n",
        "        self.shuffle = False\n",
        "        self.num_workers = 4\n",
        "        self.pin_memory = False\n",
        "\n",
        "        # Training\n",
        "        self.start_epoch = 1\n",
        "        self.end_epoch = 100\n",
        "        self.lr = 0.0002\n",
        "        self.eval_freq = 1   # eval frequency\n",
        "        self.log_freq = 1    # log training losses etc interval\n",
        "        self.save_freq = 10  # save training model interval\n",
        "\n",
        "        # model\n",
        "        self.generator_config = None\n",
        "        self.discriminator_config = None\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer_beta1 = 0.5\n",
        "        self.optimizer_beta2 = 0.999\n",
        "        \n",
        "        # Loss\n",
        "        self.l1_lambda = 100.0\n",
        "\n",
        "        self.update(*args, **kwargs)\n",
        "\n",
        "    \n",
        "    def __getitem__(self, key):\n",
        "        val = dict.__getitem__(self, key)\n",
        "        return val\n",
        "\n",
        "    def __setitem__(self, key, val):\n",
        "        dict.__setitem__(self, key, val)\n",
        "\n",
        "    def __repr__(self):\n",
        "        dictrepr = dict.__repr__(self)\n",
        "        return '%s(%s)' % (type(self).__name__, dictrepr)\n",
        "        \n",
        "    def update(self, *args, **kwargs):\n",
        "        for k, v in dict(*args, **kwargs).items():\n",
        "            self[k] = v\n",
        "\n",
        "generator_config = {\n",
        "    'in_channels': 3,\n",
        "    'out_channels': 3,\n",
        "    'blocks': [\n",
        "    {\n",
        "        'filters': 64,\n",
        "        'dropout': False,\n",
        "        'skip_connection': True\n",
        "    }, \n",
        "    {\n",
        "        'filters': 128,\n",
        "        'dropout': False,\n",
        "        'skip_connection': True\n",
        "    }, \n",
        "    {\n",
        "        'filters': 256,\n",
        "        'dropout': False,\n",
        "        'skip_connection': True\n",
        "    }, \n",
        "    {\n",
        "        'filters': 512,\n",
        "        'dropout': False,\n",
        "        'skip_connection': True\n",
        "    }, \n",
        "    {\n",
        "        'filters': 512,\n",
        "        'dropout': True,\n",
        "        'skip_connection': True\n",
        "    }, \n",
        "    {\n",
        "        'filters': 512,\n",
        "        'dropout': True,\n",
        "        'skip_connection': True\n",
        "    }, \n",
        "    {\n",
        "        'filters': 512,\n",
        "        'dropout': True,\n",
        "        'skip_connection': False\n",
        "    }]\n",
        "}\n",
        "\n",
        "discriminator_config = {\n",
        "    'in_channels': 6,  # conditionalGAN takes both real and fake image\n",
        "    'blocks': [\n",
        "    {\n",
        "        'filters': 64,\n",
        "    }, \n",
        "    {\n",
        "        'filters': 128,\n",
        "    }, \n",
        "    {\n",
        "        'filters': 256,\n",
        "    }, \n",
        "    {\n",
        "        'filters': 512,\n",
        "    }]\n",
        "}\n",
        "\n",
        "\n",
        "sconfig = SessionConfig(generator_config, discriminator_config)\n",
        "\n",
        "print(f'RUN_ID: {sconfig.run_id}')\n",
        "print(f'RANDOM_SEED: {sconfig.random_seed}')\n",
        "print(f'WORKING_FOLDER: {sconfig.working_folder}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K82GrWHp7ffE",
        "outputId": "9c802b5b-cf6f-4f9a-956d-d68a5b28f828"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUN_ID: 2022-05-27-Friday-13h-45m-22s\n",
            "RANDOM_SEED: 42\n",
            "WORKING_FOLDER: MasterProject\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup `save_file` and `load_file` for saving checkpoint"
      ],
      "metadata": {
        "id": "rHkK3LY8xERr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import files, drive\n",
        "\n",
        "    drive_dir = '/content/drive'\n",
        "    drive.mount(drive_dir)\n",
        "\n",
        "    working_dir = os.path.join(drive_dir, 'My Drive', sconfig.working_folder)\n",
        "    Path(working_dir).mkdir(parents=True, exist_ok=True)  # create directory if not exists on google drive\n",
        "\n",
        "    def save_file(file_name):\n",
        "        # save locally\n",
        "        files.download(file_name)  \n",
        "\n",
        "        # save on google drive\n",
        "        with open(file_name, 'rb') as src_file:\n",
        "            with open(os.path.join(working_dir, file_name), 'wb') as dest_file:\n",
        "                dest_file.write(src_file.read())\n",
        "    \n",
        "    def load_file(file_name):\n",
        "        if os.path.isfile(file_name):\n",
        "            print(f'\"{file_name}\" already exists, not downloading')\n",
        "            return\n",
        "        !cp f'{os.path.join(working_dir, file_name)}' file_name\n",
        "\n",
        "\n",
        "else:\n",
        "    from pydrive2.auth import GoogleAuth\n",
        "    from pydrive2.drive import GoogleDrive\n",
        "\n",
        "    def ensure_folder_on_drive(drive, folder_name):\n",
        "        folders = drive.ListFile({\n",
        "            # see https://developers.google.com/drive/api/guides/search-files\n",
        "            'q': \"mimeType = 'application/vnd.google-apps.folder'\"\n",
        "        }).GetList()\n",
        "\n",
        "        folders = list(filter(lambda folder: folder['title'] == folder_name, folders))\n",
        "\n",
        "        if len(folders) == 1:\n",
        "            return folders[0]\n",
        "        \n",
        "        if len(folders) > 1:\n",
        "            pprint(folders)\n",
        "            raise AssertionError('Multiple Folders of the same name detected')\n",
        "\n",
        "        # folder not found, create a new one at root\n",
        "        print(f'Folder: {folder_name} not found, creating at root')\n",
        "\n",
        "        folder = drive.CreateFile({\n",
        "            'title': folder_name, \n",
        "            # \"parents\": [{\n",
        "            #     \"kind\": \"drive#fileLink\", \n",
        "            #     \"id\": parent_folder_id\n",
        "            # }],\n",
        "            \"mimeType\": \"application/vnd.google-apps.folder\"\n",
        "        })\n",
        "        folder.Upload()\n",
        "        return folder\n",
        "\n",
        "\n",
        "    g_auth = GoogleAuth(settings_file=sconfig.pydrive2_setting_file, http_timeout=None)\n",
        "    g_auth.LocalWebserverAuth(host_name=\"localhost\", port_numbers=None, launch_browser=True)\n",
        "    drive = GoogleDrive(g_auth)\n",
        "\n",
        "    folder = ensure_folder_on_drive(drive, sconfig.working_folder)    \n",
        "\n",
        "    def save_file(file_name):\n",
        "        file = drive.CreateFile({\n",
        "            'title': file_name,\n",
        "            'parents': [{\n",
        "                'id': folder['id']\n",
        "            }]\n",
        "        })\n",
        "        file.SetContentFile(file_name)\n",
        "        # save to google drive\n",
        "        file.Upload()\n",
        "        # save locally\n",
        "        file.GetContentFile(file_name)\n",
        "    \n",
        "    def load_file(file_name):\n",
        "        if os.path.isfile(file_name):\n",
        "            print(f'\"{file_name}\" already exists, not downloading')\n",
        "            return\n",
        "        files = drive.ListFile({\n",
        "            'q': f\"'{folder['id']}' in parents\"\n",
        "        }).GetList()\n",
        "        downloaded = 1\n",
        "        for file in files:\n",
        "            if file['title'] == file_name:\n",
        "                # download\n",
        "                drive.CreateFile({'id': file['id']}).GetContentFile(file_name)\n",
        "                break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0olvm5GkW_M",
        "outputId": "7d8d0a4d-e2d6-41a6-de4b-69c0f2e4c349"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Miscellanuous"
      ],
      "metadata": {
        "id": "Ud3hwHHDxolq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reproducibility\n",
        "random.seed(sconfig.random_seed)\n",
        "np.random.seed(sconfig.random_seed)\n",
        "torch.manual_seed(sconfig.random_seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(sconfig.random_seed)\n",
        "\n",
        "# training device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "metadata": {
        "id": "BjJfPXh5Cdaw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "601a7c00-767f-462d-ca24-a38ea05ff277"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzCYs9x75RvS",
        "outputId": "f0064ba1-03af-48d7-bd31-2998842a022e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "fVMSbpvQAOgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UnetBlock"
      ],
      "metadata": {
        "id": "SrbZsk2Nx0zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UnetBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        in_filters, out_filters,\n",
        "\n",
        "        submodule=None, \n",
        "        sub_in_filters=None, \n",
        "        sub_out_filters=None, \n",
        "        sub_skip_connection=False, \n",
        "\n",
        "        skip_connection=True, \n",
        "        dropout=nn.Dropout, \n",
        "        in_norm=nn.BatchNorm2d, out_norm=nn.BatchNorm2d, \n",
        "        in_act=nn.LeakyReLU, out_act=nn.ReLU,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if submodule is None:\n",
        "            sub_in_filters = in_filters\n",
        "            sub_out_filters = in_filters\n",
        "            sub_skip_connection = False\n",
        "        \n",
        "        conv_common_args = {\n",
        "            'kernel_size': 4, \n",
        "            'stride': 2, \n",
        "            'padding': 1,\n",
        "            'bias': in_norm.func != nn.BatchNorm2d if type(in_norm) == functools.partial else in_norm != nn.BatchNorm2d  # batch norm has bias\n",
        "        }\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # encoder\n",
        "        layers.append(nn.Conv2d(in_channels=in_filters, out_channels=sub_in_filters, **conv_common_args))\n",
        "\n",
        "        if in_norm:\n",
        "            layers.append(in_norm(sub_in_filters))\n",
        "\n",
        "        if in_act:\n",
        "            layers.append(in_act())\n",
        "\n",
        "        \n",
        "        # submodule\n",
        "        if submodule:\n",
        "            layers.append(submodule)\n",
        "\n",
        "\n",
        "        # decoder\n",
        "        if sub_skip_connection:\n",
        "            layers.append(nn.ConvTranspose2d(in_channels=sub_out_filters * 2, out_channels=out_filters, **conv_common_args))\n",
        "        else:\n",
        "            layers.append(nn.ConvTranspose2d(in_channels=sub_out_filters    , out_channels=out_filters, **conv_common_args))\n",
        "\n",
        "        if out_norm:\n",
        "            layers.append(out_norm(out_filters))\n",
        "        \n",
        "        if dropout:\n",
        "            layers.append(dropout())\n",
        "        \n",
        "        if out_act:\n",
        "            layers.append(out_act())\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "        self.skip_connection = skip_connection\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.skip_connection:\n",
        "            return torch.cat([x, self.model(x)], dim=1)\n",
        "        else:\n",
        "            return self.model(x)"
      ],
      "metadata": {
        "id": "YCKkXJ751VOF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary(\n",
        "#     UnetBlock(in_filters=64, out_filters=64, submodule=None), \n",
        "#     input_size=(16, 64, 16, 16),\n",
        "#     col_names=['output_size', 'num_params', 'mult_adds']\n",
        "# )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg7-I2tUCO05",
        "outputId": "83fa9f45-6c32-4c71-a6fb-8fcd515078a6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===================================================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #                   Mult-Adds\n",
              "===================================================================================================================\n",
              "UnetBlock                                --                        --                        --\n",
              "├─Sequential: 1-1                        [16, 64, 16, 16]          --                        --\n",
              "│    └─Conv2d: 2-1                       [16, 64, 8, 8]            65,536                    67,108,864\n",
              "│    └─BatchNorm2d: 2-2                  [16, 64, 8, 8]            128                       2,048\n",
              "│    └─LeakyReLU: 2-3                    [16, 64, 8, 8]            --                        --\n",
              "│    └─ConvTranspose2d: 2-4              [16, 64, 16, 16]          65,536                    268,435,456\n",
              "│    └─BatchNorm2d: 2-5                  [16, 64, 16, 16]          128                       2,048\n",
              "│    └─Dropout: 2-6                      [16, 64, 16, 16]          --                        --\n",
              "│    └─ReLU: 2-7                         [16, 64, 16, 16]          --                        --\n",
              "===================================================================================================================\n",
              "Total params: 131,328\n",
              "Trainable params: 131,328\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 335.55\n",
              "===================================================================================================================\n",
              "Input size (MB): 1.05\n",
              "Forward/backward pass size (MB): 5.24\n",
              "Params size (MB): 0.53\n",
              "Estimated Total Size (MB): 6.82\n",
              "==================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator"
      ],
      "metadata": {
        "id": "J6Q6uJw-x6D6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # dependency injection\n",
        "        batch_norm = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
        "        relu = functools.partial(nn.ReLU, inplace=True)\n",
        "        leaky_relu = functools.partial(nn.LeakyReLU, inplace=True, negative_slope=0.2)\n",
        "        dropout = functools.partial(nn.Dropout, p=0.5)\n",
        "        tahn = nn.Tanh\n",
        "        \n",
        "        # build model recursively inside-out\n",
        "        blocks = config['blocks'][::-1]  \n",
        "\n",
        "        self.model = None\n",
        "\n",
        "        # build innermost block\n",
        "        self.model = UnetBlock(\n",
        "            in_filters=blocks[0]['filters'], \n",
        "            out_filters=blocks[0]['filters'],\n",
        "\n",
        "            submodule=None, \n",
        "            sub_in_filters=None, \n",
        "            sub_out_filters=None,\n",
        "            sub_skip_connection=False,\n",
        "\n",
        "            skip_connection=False,\n",
        "            dropout=dropout if blocks[0]['dropout'] else None,\n",
        "            in_norm=None, out_norm=False,\n",
        "            in_act=relu, out_act=relu\n",
        "        )\n",
        "        \n",
        "        # build between blocks\n",
        "        for i, layer in enumerate(blocks[1:], 1):\n",
        "            self.model = UnetBlock(\n",
        "                in_filters=layer['filters'], \n",
        "                out_filters=layer['filters'],\n",
        "\n",
        "                submodule=self.model, \n",
        "                sub_in_filters=blocks[i-1]['filters'], \n",
        "                sub_out_filters=blocks[i-1]['filters'],\n",
        "                sub_skip_connection=blocks[i-1]['skip_connection'],\n",
        "\n",
        "                skip_connection=blocks[i]['skip_connection'],\n",
        "                dropout=dropout if layer['dropout'] else None,\n",
        "                in_norm=batch_norm, out_norm=batch_norm,\n",
        "                in_act=leaky_relu, out_act=relu\n",
        "            )\n",
        "        \n",
        "        # build outermost block\n",
        "        self.model = UnetBlock(\n",
        "            in_filters=config['in_channels'],\n",
        "            out_filters=config['out_channels'],\n",
        "\n",
        "            submodule=self.model,\n",
        "            sub_in_filters=blocks[-1]['filters'], \n",
        "            sub_out_filters=blocks[-1]['filters'],\n",
        "            sub_skip_connection=blocks[-1]['skip_connection'],\n",
        "\n",
        "            skip_connection=False, \n",
        "            dropout=None,\n",
        "            in_norm=None, out_norm=None,\n",
        "            in_act=leaky_relu, out_act=tahn\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "7xqfiSdVxoaR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary(\n",
        "#     Generator(generator_config),\n",
        "#     input_size=(16, 3, 256, 256),\n",
        "#     col_names=['output_size', 'num_params', 'mult_adds'],\n",
        "#     depth=24\n",
        "# )"
      ],
      "metadata": {
        "id": "4CqJ7TvhgF68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator"
      ],
      "metadata": {
        "id": "QVKGbDZQx-yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # we do not use bias in conv2d layer if batch norm is used, because batch norm already has bias\n",
        "        batch_norm = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
        "        leaky_relu = functools.partial(nn.LeakyReLU, negative_slope=0.2, inplace=True)\n",
        "\n",
        "        conv_common_args = {\n",
        "            'kernel_size': 4, \n",
        "            'padding': 1,\n",
        "        }\n",
        "\n",
        "        blocks = config['blocks']\n",
        "        layers = []\n",
        "\n",
        "        # build first block\n",
        "        layers += [\n",
        "            nn.Conv2d(config['in_channels'], blocks[0]['filters'], stride=2, **conv_common_args),\n",
        "            leaky_relu()\n",
        "        ]\n",
        "\n",
        "        # build between block\n",
        "        prev_filters = blocks[0]['filters']\n",
        "        for i, layer in enumerate(blocks[1:-1], 1):\n",
        "            curr_filters = min(blocks[i]['filters'], blocks[0]['filters']*8)\n",
        "            layers += [\n",
        "                nn.Conv2d(prev_filters, curr_filters, bias=False, stride=2, **conv_common_args),\n",
        "                batch_norm(curr_filters),\n",
        "                leaky_relu()\n",
        "            ]\n",
        "            prev_filters = curr_filters\n",
        "\n",
        "        # build last block\n",
        "        curr_filters = min(blocks[-1]['filters'], blocks[0]['filters'] * 8)\n",
        "        layers += [\n",
        "            # stride = 1 for last block\n",
        "            nn.Conv2d(prev_filters, curr_filters, stride=1, bias=False, **conv_common_args),\n",
        "            batch_norm(curr_filters),\n",
        "            leaky_relu(),\n",
        "            # convert to 1 dimensional output\n",
        "            nn.Conv2d(curr_filters, 1, stride=1, **conv_common_args)\n",
        "        ]\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "bTMsSqaA5lSj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary(\n",
        "#     Discriminator(discriminator_config),\n",
        "#     input_size=(16, 3, 256, 256),\n",
        "#     col_names=['output_size', 'num_params', 'mult_adds'],\n",
        "#     depth=24\n",
        "# )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptgC-cZVQJrZ",
        "outputId": "8ed0eaf1-3baa-4e14-ddfa-4ac9b398ee4f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===================================================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #                   Mult-Adds\n",
              "===================================================================================================================\n",
              "Discriminator                            --                        --                        --\n",
              "├─Sequential: 1-1                        [16, 1, 30, 30]           --                        --\n",
              "│    └─Conv2d: 2-1                       [16, 64, 128, 128]        3,136                     822,083,584\n",
              "│    └─LeakyReLU: 2-2                    [16, 64, 128, 128]        --                        --\n",
              "│    └─Conv2d: 2-3                       [16, 128, 64, 64]         131,072                   8,589,934,592\n",
              "│    └─BatchNorm2d: 2-4                  [16, 128, 64, 64]         256                       4,096\n",
              "│    └─LeakyReLU: 2-5                    [16, 128, 64, 64]         --                        --\n",
              "│    └─Conv2d: 2-6                       [16, 256, 32, 32]         524,288                   8,589,934,592\n",
              "│    └─BatchNorm2d: 2-7                  [16, 256, 32, 32]         512                       8,192\n",
              "│    └─LeakyReLU: 2-8                    [16, 256, 32, 32]         --                        --\n",
              "│    └─Conv2d: 2-9                       [16, 512, 31, 31]         2,097,152                 32,245,809,152\n",
              "│    └─BatchNorm2d: 2-10                 [16, 512, 31, 31]         1,024                     16,384\n",
              "│    └─LeakyReLU: 2-11                   [16, 512, 31, 31]         --                        --\n",
              "│    └─Conv2d: 2-12                      [16, 1, 30, 30]           8,193                     117,979,200\n",
              "===================================================================================================================\n",
              "Total params: 2,765,633\n",
              "Trainable params: 2,765,633\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 50.37\n",
              "===================================================================================================================\n",
              "Input size (MB): 12.58\n",
              "Forward/backward pass size (MB): 461.62\n",
              "Params size (MB): 11.06\n",
              "Estimated Total Size (MB): 485.27\n",
              "==================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "_c_T8k_F_1-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "FILE=\"facades\"\n",
        "\n",
        "if [[ $FILE != \"cityscapes\" && $FILE != \"night2day\" && $FILE != \"edges2handbags\" && $FILE != \"edges2shoes\" && $FILE != \"facades\" && $FILE != \"maps\" ]]; then\n",
        "  echo \"Available datasets are cityscapes, night2day, edges2handbags, edges2shoes, facades, maps\"\n",
        "  exit 1\n",
        "fi\n",
        "\n",
        "if [[ $FILE == \"cityscapes\" ]]; then\n",
        "    echo \"Due to license issue, we cannot provide the Cityscapes dataset from our repository. Please download the Cityscapes dataset from https://cityscapes-dataset.com, and use the script ./datasets/prepare_cityscapes_dataset.py.\"\n",
        "    echo \"You need to download gtFine_trainvaltest.zip and leftImg8bit_trainvaltest.zip. For further instruction, please read ./datasets/prepare_cityscapes_dataset.py\"\n",
        "    exit 1\n",
        "fi\n",
        "\n",
        "echo \"Specified [$FILE]\"\n",
        "\n",
        "URL=http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/$FILE.tar.gz\n",
        "TAR_FILE=./datasets/$FILE.tar.gz\n",
        "TARGET_DIR=./datasets/$FILE/\n",
        "wget -N $URL -O $TAR_FILE > /dev/null\n",
        "mkdir -p $TARGET_DIR\n",
        "tar -zxvf $TAR_FILE -C ./datasets/ > /dev/null\n",
        "rm $TAR_FILE"
      ],
      "metadata": {
        "id": "JMHnAibAA5vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ],
      "metadata": {
        "id": "FyDfiYMkyHmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_EXTENSIONS = [\n",
        "    '.jpg', '.jpeg',\n",
        "    '.png', '.ppm', '.bmp',\n",
        "    '.tif', '.tiff',\n",
        "]\n",
        "\n",
        "def is_image_file(filename):\n",
        "    return any(filename.lower().endswith(ext) for ext in IMG_EXTENSIONS)\n",
        "\n",
        "def get_all_image_paths(root):\n",
        "    paths = []\n",
        "    assert os.path.isdir(root)\n",
        "\n",
        "    for root, _folders, filenames in sorted(os.walk(root)):\n",
        "        for filename in filenames:\n",
        "            if is_image_file(filename):\n",
        "                paths.append(os.path.join(root, filename))\n",
        "    \n",
        "    return paths"
      ],
      "metadata": {
        "id": "emuZ1ZgzFUut"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "4FFPAVn8yP8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root):\n",
        "        self.paths = sorted(get_all_image_paths(root))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        A, B = self._split_input_output(self._read_im(self.paths[i]))\n",
        "\n",
        "        transform = self._generate_transform()\n",
        "        \n",
        "        return transform(A), transform(B)\n",
        "\n",
        "    def _read_im(self, path):\n",
        "        return Image.open(path).convert('RGB')\n",
        "\n",
        "    def _split_input_output(AB):\n",
        "        w, h = AB.size\n",
        "        w2 = int(w / 2)\n",
        "        A = AB.crop((0, 0, w2, h))\n",
        "        B = AB.crop((w2, 0, w, h))\n",
        "\n",
        "        return A, B\n",
        "\n",
        "    def _generate_transform(self):\n",
        "        new_size = 286\n",
        "        old_size = 256\n",
        "\n",
        "        rand_x = random.randint(0, new_size - old_size)\n",
        "        rand_y = random.randint(0, new_size - old_size)\n",
        "        flip = random.random() > 0.5\n",
        "        \n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((new_size, new_size), interpolation=InterpolationMode.BICUBIC, antialias=True),\n",
        "            transforms.Lambda(lambda im: self._crop(im, (rand_x, rand_y), (old_size, old_size))),\n",
        "            transforms.Lambda(lambda im: self._flip(im, flip)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "\n",
        "    def _flip(self, im, flip):\n",
        "        if flip:\n",
        "            return im.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        return im\n",
        "\n",
        "    def _crop(self, im, pos, size):\n",
        "        return im.crop((pos[0], pos[1], pos[0] + size[0], pos[1] + size[1]))"
      ],
      "metadata": {
        "id": "gab1vj-XH3UV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criterion"
      ],
      "metadata": {
        "id": "_hHFHrTkC_AN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GANLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, real_label=1.0, fake_label=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.register_buffer('real_label', torch.tensor(real_label))\n",
        "        self.register_buffer('fake_label', torch.tensor(fake_label))\n",
        "\n",
        "        self.loss = nn.MSELoss()\n",
        "    \n",
        "    def __call__(self, model_output, target_is_real):\n",
        "\n",
        "        label = self.real_label if target_is_real else self.fake_label\n",
        "        label.expand_as(model_output)\n",
        "\n",
        "        return self.loss(model_output, label)"
      ],
      "metadata": {
        "id": "OdhEYBt5DBIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "MZ6AD7e2mN7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ],
      "metadata": {
        "id": "k8DHdOUqp5Hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_time(seconds):\n",
        "    return time.strftime('%Hh:%Mm:%Ss', time.gmtime(seconds))\n",
        "\n",
        "def save_checkpoint(net_G, net_D, optimizer_G, optimizer_D, epoch):\n",
        "    file_name = f'{sconfig.run_id}_epoch_{epoch}.ckpt'\n",
        "    torch.save({\n",
        "        'net_G_state_dict': net_G.state_dict(),\n",
        "        'net_D_state_dict': net_D.state_dict(),\n",
        "        'net_G_optimizer_state_dict': optimizer_G.state_dict(),\n",
        "        'net_D_optimizer_state_dict': optimizer_D.state_dict(),\n",
        "        'session_config': sconfig,\n",
        "        'epoch': epoch\n",
        "    }, file_name)\n",
        "\n",
        "def load_checkpoint(run_id, epoch):\n",
        "    file_name = f'{run_id}_epoch_{epoch}.ckpt'\n",
        "    load_file(file_name)  # ensure exists locally\n",
        "    checkpoint = torch.load(file_name)\n",
        "\n",
        "    sconfig = checkpoint['session_config']\n",
        "\n",
        "    net_G = Generator(sconfig.generator_config)\n",
        "    net_D = Discriminator(sconfig.discriminator_config)\n",
        "    net_G.load_state_dict(checkpoint['net_G_state_dict'])\n",
        "    net_D.load_state_dict(checkpoint['net_D_state_dict'])\n",
        "\n",
        "    optimizer_G = optim.Adam(net_G.parameters(), lr=sconfig.lr, betas=(sconfig.optimizer_beta1, sconfig.optimizer_beta2))\n",
        "    optimizer_D = optim.Adam(net_D.parameters(), lr=sconfig.lr, betas=(sconfig.optimizer_beta1, sconfig.optimizer_beta2))\n",
        "    optimizer_G.load_state_dict(checkpoint['net_G_optimizer_state_dict'])\n",
        "    optimizer_D.load_state_dict(checkpoint['net_D_optimizer_state_dict'])\n",
        "\n",
        "    return sconfig, net_G, net_D, optimizer_G, optimizer_D, checkpoint['epoch']\n",
        "\n",
        "def set_requires_grad(net, requires_grad):\n",
        "    for param in net.parameters():\n",
        "        param.requires_grad = requires_grad\n"
      ],
      "metadata": {
        "id": "nQziH1Lir5tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_batch(net_G, net_D, optimizer_G, optimizer_D, real_A, real_B, criterion_gan, criterion_l1):\n",
        "\n",
        "    ###\n",
        "    # discrminator\n",
        "    ###\n",
        "    set_requires_grad(net_D, True)\n",
        "\n",
        "    # generate fake image using generator\n",
        "    fake_B = net_G(real_A)\n",
        "\n",
        "    # discrminate fake image\n",
        "    fake_AB = torch.cat((real_A, fake_B), dim=1).detach()  # conditionalGAN takes both real and fake image\n",
        "    pred_fake = net_D(fake_AB)\n",
        "    loss_D_fake = criterion_gan(pred_fake, False)\n",
        "\n",
        "    # discrminate real image\n",
        "    real_AB = torch.cat((real_A, real_B), 1)\n",
        "    pred_real = net_D(real_AB)\n",
        "    loss_D_real = criterion_gan(pred_real, True)\n",
        "\n",
        "    # update\n",
        "    optimizer_D.zero_grad()\n",
        "    loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
        "    loss_D.backward()\n",
        "    optimizer_D.step()\n",
        "\n",
        "    ###\n",
        "    # generator\n",
        "    ###\n",
        "    set_requires_grad(net_D, False)\n",
        "\n",
        "    # generator should fool the discriminator\n",
        "    fake_AB = torch.cat((real_A, fake_B), 1)\n",
        "    pred_fake = net_D(fake_AB)\n",
        "    loss_G_fake = criterion_gan(pred_fake, True)\n",
        "\n",
        "    # l1 loss between generated and real image for more accurate output\n",
        "    loss_G_l1 = criterion_l1(fake_B, real_B) * sconfig.l1_lambda\n",
        "\n",
        "    # update\n",
        "    optimizer_G.zero_grad()\n",
        "    loss_G = loss_G_fake + loss_G_l1\n",
        "    loss_G.backward()\n",
        "    optimizer_G.step()\n",
        "\n",
        "    return loss_G.item(), loss_D.item()"
      ],
      "metadata": {
        "id": "YsBcPbyyyMmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MyDataset('./datasets/facades/train')\n",
        "dataloader = DataLoader(dataset, batch_size=sconfig.batch_size, shuffle=sconfig.shuffle, num_workers=sconfig.num_workers, pin_memory=sconfig.pin_memory)\n",
        "\n",
        "net_G = Generator(generator_config)\n",
        "net_D = Discriminator(discriminator_config)\n",
        "\n",
        "criterion_l1 = nn.L1Loss()\n",
        "criterion_gan = GANLoss()\n",
        "\n",
        "optimizer_G = optim.Adam(net_G.parameters(), lr=sconfig.lr, betas=(sconfig.optimizer_beta1, sconfig.optimizer_beta2))\n",
        "optimizer_D = optim.Adam(net_D.parameters(), lr=sconfig.lr, betas=(sconfig.optimizer_beta1, sconfig.optimizer_beta2))\n",
        "\n",
        "net_G.to(device)\n",
        "net_D.to(device)\n",
        "\n",
        "training_start_time = time.time()\n",
        "for epoch in range(sconfig.start_epoch, sconfig.end_epoch + 1):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    epoch_train_G_losses = []\n",
        "    epoch_train_D_losses = []\n",
        "    for i, (inp, tar) in enumerate(dataloader):\n",
        "        net_G.train()\n",
        "        net_D.train()\n",
        "        inp = inp.to(device)\n",
        "        tar = tar.to(device)\n",
        "\n",
        "        loss_G, loss_D = train_batch(net_G, net_D, inp, tar)\n",
        "\n",
        "        epoch_train_G_losses.append(loss_G)\n",
        "        epoch_train_D_losses.append(loss_D)\n",
        "    \n",
        "    if epoch % sconfig.eval_freq == 0 or epoch == sconfig.start_epoch:\n",
        "        net_G.eval()\n",
        "        net_D.eval()\n",
        "        epoch_eval_loss = ...\n",
        "    else:\n",
        "        epoch_eval_loss = None\n",
        "    \n",
        "    if epoch % sconfig.log_freq == 0 or epoch == sconfig.start_epoch:\n",
        "        print(\n",
        "            f'[Epoch={epoch}] ' \\\n",
        "            f'[TrainLossG={np.mean(epoch_train_G_losses):.4f}] ' \\\n",
        "            f'[TrainLossD={np.mean(epoch_train_D_losses):.4f}] ' \\\n",
        "            f'[EvalLoss={epoch_eval_loss:.4f}]' if epoch_eval_loss is not None else '' \\\n",
        "            f'[EpochTime={format_time(time.time() - epoch_start_time)}] ' \\\n",
        "            f'[TrainTime={format_time(time.time() - training_start_time)}]'\n",
        "        )\n",
        "\n",
        "    if epoch % sconfig.save_freq == 0 or epoch == sconfig.start_epoch:\n",
        "        save_checkpoint(net_G, net_D, optimizer_G, optimizer_D, epoch)"
      ],
      "metadata": {
        "id": "0oNbNptimW2p"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "IX1osH6sObRt"
      }
    }
  ]
}
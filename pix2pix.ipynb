{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pix2pix.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rHkK3LY8xERr",
        "Ud3hwHHDxolq",
        "UVdSP7BCPgCk"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPYSaMBxxG70OQbH05DTeSV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcxx/ucl-master-project/blob/master/pix2pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Settings"
      ],
      "metadata": {
        "id": "247rtPpgAHwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "DjZs_l1Pz0Ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pydrive2\n",
        "%pip install torchinfo"
      ],
      "metadata": {
        "id": "b6AcNCCGK8nn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fbff85f-1cf0-4a3a-94b0-61699180504b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydrive2\n",
            "  Downloading PyDrive2-1.10.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.7/dist-packages (from pydrive2) (3.13)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from pydrive2) (1.15.0)\n",
            "Collecting pyOpenSSL>=19.1.0\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pydrive2) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.12.5 in /usr/local/lib/python3.7/dist-packages (from pydrive2) (1.12.11)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.12.5->pydrive2) (0.0.4)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.12.5->pydrive2) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.12.5->pydrive2) (1.31.5)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.12.5->pydrive2) (3.0.1)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.12.5->pydrive2) (1.35.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.12.5->pydrive2) (1.56.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.12.5->pydrive2) (57.4.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.12.5->pydrive2) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.12.5->pydrive2) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.12.5->pydrive2) (2022.1)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.12.5->pydrive2) (3.17.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.12.5->pydrive2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.12.5->pydrive2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.12.5->pydrive2) (4.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->pydrive2) (0.4.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.12.5->pydrive2) (3.0.9)\n",
            "Collecting cryptography>=35.0\n",
            "  Downloading cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 18.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=35.0->pyOpenSSL>=19.1.0->pydrive2) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=35.0->pyOpenSSL>=19.1.0->pydrive2) (2.21)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.12.5->pydrive2) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.12.5->pydrive2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.12.5->pydrive2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.12.5->pydrive2) (2.10)\n",
            "Installing collected packages: cryptography, pyOpenSSL, pydrive2\n",
            "Successfully installed cryptography-37.0.2 pyOpenSSL-22.0.0 pydrive2-1.10.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.6.6-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.6.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import functools\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "id": "2kQXwgfm5sVP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "ULAvvWgOvvuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SessionConfig(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.__dict__ = self  # dangerous, merging namespace, but now you can access key using .key instead of ['key']\n",
        "\n",
        "        # House keeping\n",
        "        self.run_id = datetime.now().strftime('%Y-%m-%d-%A-%Hh-%Mm-%Ss')\n",
        "        self.random_seed = 42\n",
        "        self.working_folder = 'MasterProject'  # will be created on google drive at root\n",
        "        self.pydrive2_setting_file = 'settings.yaml'\n",
        "\n",
        "        # Dataset\n",
        "        self.batch_size = 1\n",
        "        self.shuffle = False\n",
        "        self.num_workers = 4\n",
        "        self.pin_memory = True\n",
        "        self.A_to_B = False\n",
        "\n",
        "        # Training\n",
        "        self.start_epoch = 1\n",
        "        self.end_epoch = 300\n",
        "        self.eval_freq = 1   # eval frequency, unit epoch\n",
        "        self.log_freq = 1    # log frequency, unit epoch\n",
        "        self.save_freq = 20  # save checkpoint, unit epoch\n",
        "\n",
        "        # Model\n",
        "        self.generator_config = None\n",
        "        self.discriminator_config = None\n",
        "\n",
        "        # Optimizer\n",
        "        self.lr = 0.0002\n",
        "        self.optimizer_beta1 = 0.5\n",
        "        self.optimizer_beta2 = 0.999\n",
        "        self.init_gain = 0.02\n",
        "\n",
        "        # Scheduler\n",
        "        self.epochs_decay = 100\n",
        "        \n",
        "        # Loss\n",
        "        self.l1_lambda = 100.0    # encourage l1 distance to actual output\n",
        "        self.d_loss_factor = 0.5  # to slow down discrminator learning\n",
        "\n",
        "        self.update(dict(*args, **kwargs))\n",
        "\n",
        "generator_config = {\n",
        "    'in_channels': 3,\n",
        "    'out_channels': 3,\n",
        "    'blocks': [\n",
        "    {\n",
        "        'filters': 64,\n",
        "        'dropout': False,\n",
        "        'skip_connection': False\n",
        "    }, \n",
        "    {\n",
        "        'filters': 128,\n",
        "        'dropout': False,\n",
        "        'skip_connection': True\n",
        "    }, \n",
        "    {\n",
        "        'filters': 256,\n",
        "        'dropout': False,\n",
        "        'skip_connection': True\n",
        "    }, \n",
        "    {\n",
        "        'filters': 512,\n",
        "        'dropout': True,\n",
        "        'skip_connection': True\n",
        "    }, \n",
        "    {\n",
        "        'filters': 512,\n",
        "        'dropout': True,\n",
        "        'skip_connection': True\n",
        "    }, \n",
        "    {\n",
        "        'filters': 512,\n",
        "        'dropout': True,\n",
        "        'skip_connection': True\n",
        "    }, \n",
        "    {\n",
        "        'filters': 512,\n",
        "        'dropout': False,\n",
        "        'skip_connection': True\n",
        "    }]\n",
        "}\n",
        "\n",
        "discriminator_config = {\n",
        "    'in_channels': 3 * 2,  # conditionalGAN takes both real and fake image\n",
        "    'blocks': [\n",
        "    {\n",
        "        'filters': 64,\n",
        "    }, \n",
        "    {\n",
        "        'filters': 128,\n",
        "    }, \n",
        "    {\n",
        "        'filters': 256,\n",
        "    }, \n",
        "    {\n",
        "        'filters': 512,\n",
        "    }]\n",
        "}\n",
        "\n",
        "\n",
        "sconfig = SessionConfig({\n",
        "    'generator_config': generator_config,\n",
        "    'discriminator_config':discriminator_config,\n",
        "})\n",
        "\n",
        "pprint(sconfig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K82GrWHp7ffE",
        "outputId": "3067d63c-136e-4a93-c8a6-3b4df134e5dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'A_to_B': False,\n",
            " 'batch_size': 1,\n",
            " 'd_loss_factor': 0.5,\n",
            " 'discriminator_config': {'blocks': [{'filters': 64},\n",
            "                                     {'filters': 128},\n",
            "                                     {'filters': 256},\n",
            "                                     {'filters': 512}],\n",
            "                          'in_channels': 6},\n",
            " 'end_epoch': 300,\n",
            " 'epochs_decay': 100,\n",
            " 'eval_freq': 1,\n",
            " 'generator_config': {'blocks': [{'dropout': False,\n",
            "                                  'filters': 64,\n",
            "                                  'skip_connection': False},\n",
            "                                 {'dropout': False,\n",
            "                                  'filters': 128,\n",
            "                                  'skip_connection': True},\n",
            "                                 {'dropout': False,\n",
            "                                  'filters': 256,\n",
            "                                  'skip_connection': True},\n",
            "                                 {'dropout': True,\n",
            "                                  'filters': 512,\n",
            "                                  'skip_connection': True},\n",
            "                                 {'dropout': True,\n",
            "                                  'filters': 512,\n",
            "                                  'skip_connection': True},\n",
            "                                 {'dropout': True,\n",
            "                                  'filters': 512,\n",
            "                                  'skip_connection': True},\n",
            "                                 {'dropout': False,\n",
            "                                  'filters': 512,\n",
            "                                  'skip_connection': True}],\n",
            "                      'in_channels': 3,\n",
            "                      'out_channels': 3},\n",
            " 'init_gain': 0.02,\n",
            " 'l1_lambda': 100.0,\n",
            " 'log_freq': 1,\n",
            " 'lr': 0.0002,\n",
            " 'num_workers': 4,\n",
            " 'optimizer_beta1': 0.5,\n",
            " 'optimizer_beta2': 0.999,\n",
            " 'pin_memory': True,\n",
            " 'pydrive2_setting_file': 'settings.yaml',\n",
            " 'random_seed': 42,\n",
            " 'run_id': '2022-05-28-Saturday-21h-27m-21s',\n",
            " 'save_freq': 20,\n",
            " 'shuffle': False,\n",
            " 'start_epoch': 1,\n",
            " 'working_folder': 'MasterProject'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and Loading"
      ],
      "metadata": {
        "id": "rHkK3LY8xERr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import files, drive\n",
        "\n",
        "    drive_dir = '/content/drive'\n",
        "    drive.mount(drive_dir)\n",
        "\n",
        "    working_dir = os.path.join(drive_dir, 'My Drive', sconfig.working_folder, sconfig.run_id)\n",
        "    print(working_dir)\n",
        "    Path(working_dir).mkdir(parents=True, exist_ok=True)  # create directory if not exists on google drive\n",
        "\n",
        "    def save_file(file_name, local=True):\n",
        "        # save locally\n",
        "        if local:\n",
        "            files.download(file_name)  \n",
        "\n",
        "        # save on google drive\n",
        "        with open(file_name, 'rb') as src_file:\n",
        "            with open(os.path.join(working_dir, file_name), 'wb') as dest_file:\n",
        "                dest_file.write(src_file.read())\n",
        "    \n",
        "    def load_file(file_name):\n",
        "        if os.path.isfile(file_name):\n",
        "            print(f'\"{file_name}\" already exists, not downloading')\n",
        "            return\n",
        "        !cp f'{os.path.join(working_dir, file_name)}' file_name\n",
        "\n",
        "\n",
        "else:\n",
        "    from pydrive2.auth import GoogleAuth\n",
        "    from pydrive2.drive import GoogleDrive\n",
        "\n",
        "    def ensure_folder_on_drive(drive, folder_name):\n",
        "        folders = drive.ListFile({\n",
        "            # see https://developers.google.com/drive/api/guides/search-files\n",
        "            'q': \"mimeType = 'application/vnd.google-apps.folder'\"\n",
        "        }).GetList()\n",
        "\n",
        "        folders = list(filter(lambda folder: folder['title'] == folder_name, folders))\n",
        "\n",
        "        if len(folders) == 1:\n",
        "            return folders[0]\n",
        "        \n",
        "        if len(folders) > 1:\n",
        "            pprint(folders)\n",
        "            raise AssertionError('Multiple Folders of the same name detected')\n",
        "\n",
        "        # folder not found, create a new one at root\n",
        "        print(f'Folder: {folder_name} not found, creating at root')\n",
        "\n",
        "        folder = drive.CreateFile({\n",
        "            'title': folder_name, \n",
        "            # \"parents\": [{\n",
        "            #     \"kind\": \"drive#fileLink\", \n",
        "            #     \"id\": parent_folder_id\n",
        "            # }],\n",
        "            \"mimeType\": \"application/vnd.google-apps.folder\"\n",
        "        })\n",
        "        folder.Upload()\n",
        "        return folder\n",
        "\n",
        "\n",
        "    g_auth = GoogleAuth(settings_file=sconfig.pydrive2_setting_file, http_timeout=None)\n",
        "    g_auth.LocalWebserverAuth(host_name=\"localhost\", port_numbers=None, launch_browser=True)\n",
        "    drive = GoogleDrive(g_auth)\n",
        "\n",
        "    folder = ensure_folder_on_drive(drive, sconfig.working_folder)    \n",
        "\n",
        "    def save_file(file_name, local=True):\n",
        "        file = drive.CreateFile({\n",
        "            'title': file_name,\n",
        "            'parents': [{\n",
        "                'id': folder['id']\n",
        "            }]\n",
        "        })\n",
        "        file.SetContentFile(file_name)\n",
        "        # save to google drive\n",
        "        file.Upload()\n",
        "        # save locally\n",
        "        if local:\n",
        "            file.GetContentFile(file_name)\n",
        "    \n",
        "    def load_file(file_name):\n",
        "        if os.path.isfile(file_name):\n",
        "            print(f'\"{file_name}\" already exists, not downloading')\n",
        "            return\n",
        "        files = drive.ListFile({\n",
        "            'q': f\"'{folder['id']}' in parents\"\n",
        "        }).GetList()\n",
        "        downloaded = 1\n",
        "        for file in files:\n",
        "            if file['title'] == file_name:\n",
        "                # download\n",
        "                drive.CreateFile({'id': file['id']}).GetContentFile(file_name)\n",
        "                break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0olvm5GkW_M",
        "outputId": "e5f9d38d-2676-4527-f1ef-025d2e2df49e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/MasterProject/2022-05-28-Saturday-21h-27m-21s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reproducibility"
      ],
      "metadata": {
        "id": "Ud3hwHHDxolq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(sconfig.random_seed)\n",
        "np.random.seed(sconfig.random_seed)\n",
        "torch.manual_seed(sconfig.random_seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(sconfig.random_seed)"
      ],
      "metadata": {
        "id": "BjJfPXh5Cdaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Device"
      ],
      "metadata": {
        "id": "UVdSP7BCPgCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "metadata": {
        "id": "P5tJMhBzPehG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzCYs9x75RvS"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "fVMSbpvQAOgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UnetBlock"
      ],
      "metadata": {
        "id": "SrbZsk2Nx0zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UnetBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        in_filters, out_filters,\n",
        "\n",
        "        submodule=None, \n",
        "        sub_in_filters=None, \n",
        "        sub_out_filters=None, \n",
        "        sub_skip_connection=False, \n",
        "\n",
        "        skip_connection=True, \n",
        "        dropout=nn.Dropout, \n",
        "        in_norm=nn.BatchNorm2d, out_norm=nn.BatchNorm2d, \n",
        "        in_act=nn.LeakyReLU, out_act=nn.ReLU,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if submodule is None:\n",
        "            sub_in_filters = in_filters\n",
        "            sub_out_filters = in_filters\n",
        "            sub_skip_connection = False\n",
        "        \n",
        "        conv_common_args = {\n",
        "            'kernel_size': 4, \n",
        "            'stride': 2, \n",
        "            'padding': 1,\n",
        "            'bias': in_norm.func != nn.BatchNorm2d if type(in_norm) == functools.partial else in_norm != nn.BatchNorm2d  # batch norm has bias\n",
        "        }\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # encoder\n",
        "        layers.append(nn.Conv2d(in_channels=in_filters, out_channels=sub_in_filters, **conv_common_args))\n",
        "\n",
        "        if in_norm:\n",
        "            layers.append(in_norm(sub_in_filters))\n",
        "\n",
        "        if in_act:\n",
        "            layers.append(in_act())\n",
        "\n",
        "        \n",
        "        # submodule\n",
        "        if submodule:\n",
        "            layers.append(submodule)\n",
        "\n",
        "\n",
        "        # decoder\n",
        "        if sub_skip_connection:\n",
        "            layers.append(nn.ConvTranspose2d(in_channels=sub_out_filters * 2, out_channels=out_filters, **conv_common_args))\n",
        "        else:\n",
        "            layers.append(nn.ConvTranspose2d(in_channels=sub_out_filters    , out_channels=out_filters, **conv_common_args))\n",
        "\n",
        "        if out_norm:\n",
        "            layers.append(out_norm(out_filters))\n",
        "        \n",
        "        if dropout:\n",
        "            layers.append(dropout())\n",
        "        \n",
        "        if out_act:\n",
        "            layers.append(out_act())\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "        self.skip_connection = skip_connection\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.skip_connection:\n",
        "            return torch.cat([x, self.model(x)], dim=1)\n",
        "        else:\n",
        "            return self.model(x)"
      ],
      "metadata": {
        "id": "YCKkXJ751VOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary(\n",
        "#     UnetBlock(in_filters=64, out_filters=64, submodule=None), \n",
        "#     input_size=(16, 64, 16, 16),\n",
        "#     col_names=['output_size', 'num_params', 'mult_adds']\n",
        "# )"
      ],
      "metadata": {
        "id": "Mg7-I2tUCO05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator"
      ],
      "metadata": {
        "id": "J6Q6uJw-x6D6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # dependency injection\n",
        "        batch_norm = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
        "        relu = functools.partial(nn.ReLU, inplace=True)\n",
        "        leaky_relu = functools.partial(nn.LeakyReLU, inplace=True, negative_slope=0.2)\n",
        "        dropout = functools.partial(nn.Dropout, p=0.5)\n",
        "        tahn = nn.Tanh\n",
        "        \n",
        "        # build model recursively inside-out\n",
        "        blocks = config['blocks'][::-1]  \n",
        "\n",
        "        self.model = None\n",
        "\n",
        "        # build innermost block\n",
        "        self.model = UnetBlock(\n",
        "            in_filters=blocks[0]['filters'], \n",
        "            out_filters=blocks[0]['filters'],\n",
        "\n",
        "            submodule=None, \n",
        "            sub_in_filters=None, \n",
        "            sub_out_filters=None,\n",
        "            sub_skip_connection=False,\n",
        "\n",
        "            skip_connection=blocks[0]['skip_connection'],\n",
        "            dropout=dropout if blocks[0]['dropout'] else None,\n",
        "            in_norm=None, out_norm=False,\n",
        "            in_act=relu, out_act=relu\n",
        "        )\n",
        "        \n",
        "        # build between blocks\n",
        "        for i, layer in enumerate(blocks[1:], 1):\n",
        "            self.model = UnetBlock(\n",
        "                in_filters=layer['filters'], \n",
        "                out_filters=layer['filters'],\n",
        "\n",
        "                submodule=self.model, \n",
        "                sub_in_filters=blocks[i-1]['filters'], \n",
        "                sub_out_filters=blocks[i-1]['filters'],\n",
        "                sub_skip_connection=blocks[i-1]['skip_connection'],\n",
        "\n",
        "                skip_connection=blocks[i]['skip_connection'],\n",
        "                dropout=dropout if layer['dropout'] else None,\n",
        "                in_norm=batch_norm, out_norm=batch_norm,\n",
        "                in_act=leaky_relu, out_act=relu\n",
        "            )\n",
        "        \n",
        "        # build outermost block\n",
        "        self.model = UnetBlock(\n",
        "            in_filters=config['in_channels'],\n",
        "            out_filters=config['out_channels'],\n",
        "\n",
        "            submodule=self.model,\n",
        "            sub_in_filters=blocks[-1]['filters'], \n",
        "            sub_out_filters=blocks[-1]['filters'],\n",
        "            sub_skip_connection=blocks[-1]['skip_connection'],\n",
        "\n",
        "            skip_connection=blocks[-1]['skip_connection'], \n",
        "            dropout=None,\n",
        "            in_norm=None, out_norm=None,\n",
        "            in_act=leaky_relu, out_act=tahn\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "7xqfiSdVxoaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary(\n",
        "#     Generator(generator_config),\n",
        "#     input_size=(16, 3, 256, 256),\n",
        "#     col_names=['output_size', 'num_params', 'mult_adds'],\n",
        "#     depth=24\n",
        "# )\n",
        "\n",
        "print(Generator(generator_config))"
      ],
      "metadata": {
        "id": "4CqJ7TvhgF68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator"
      ],
      "metadata": {
        "id": "QVKGbDZQx-yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # we do not use bias in conv2d layer if batch norm is used, because batch norm already has bias\n",
        "        batch_norm = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
        "        leaky_relu = functools.partial(nn.LeakyReLU, negative_slope=0.2, inplace=True)\n",
        "\n",
        "        conv_common_args = {\n",
        "            'kernel_size': 4, \n",
        "            'padding': 1,\n",
        "        }\n",
        "\n",
        "        blocks = config['blocks']\n",
        "        layers = []\n",
        "\n",
        "        # build first block\n",
        "        layers += [\n",
        "            nn.Conv2d(config['in_channels'], blocks[0]['filters'], stride=2, **conv_common_args),\n",
        "            leaky_relu()\n",
        "        ]\n",
        "\n",
        "        # build between block\n",
        "        prev_filters = blocks[0]['filters']\n",
        "        for i, layer in enumerate(blocks[1:-1], 1):\n",
        "            curr_filters = min(blocks[i]['filters'], blocks[0]['filters']*8)\n",
        "            layers += [\n",
        "                nn.Conv2d(prev_filters, curr_filters, bias=False, stride=2, **conv_common_args),\n",
        "                batch_norm(curr_filters),\n",
        "                leaky_relu()\n",
        "            ]\n",
        "            prev_filters = curr_filters\n",
        "\n",
        "        # build last block\n",
        "        curr_filters = min(blocks[-1]['filters'], blocks[0]['filters'] * 8)\n",
        "        layers += [\n",
        "            # stride = 1 for last block\n",
        "            nn.Conv2d(prev_filters, curr_filters, stride=1, bias=False, **conv_common_args),\n",
        "            batch_norm(curr_filters),\n",
        "            leaky_relu(),\n",
        "            # convert to 1 dimensional output\n",
        "            nn.Conv2d(curr_filters, 1, stride=1, **conv_common_args)\n",
        "        ]\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "bTMsSqaA5lSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary(\n",
        "#     Discriminator(discriminator_config),\n",
        "#     input_size=(16, 3, 256, 256),\n",
        "#     col_names=['output_size', 'num_params', 'mult_adds'],\n",
        "#     depth=24\n",
        "# )"
      ],
      "metadata": {
        "id": "ptgC-cZVQJrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "_c_T8k_F_1-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "FILE=\"facades\"\n",
        "\n",
        "if [[ $FILE != \"cityscapes\" && $FILE != \"night2day\" && $FILE != \"edges2handbags\" && $FILE != \"edges2shoes\" && $FILE != \"facades\" && $FILE != \"maps\" ]]; then\n",
        "  echo \"Available datasets are cityscapes, night2day, edges2handbags, edges2shoes, facades, maps\"\n",
        "  exit 1\n",
        "fi\n",
        "\n",
        "if [[ $FILE == \"cityscapes\" ]]; then\n",
        "    echo \"Due to license issue, we cannot provide the Cityscapes dataset from our repository. Please download the Cityscapes dataset from https://cityscapes-dataset.com, and use the script ./datasets/prepare_cityscapes_dataset.py.\"\n",
        "    echo \"You need to download gtFine_trainvaltest.zip and leftImg8bit_trainvaltest.zip. For further instruction, please read ./datasets/prepare_cityscapes_dataset.py\"\n",
        "    exit 1\n",
        "fi\n",
        "\n",
        "echo \"Specified [$FILE]\"\n",
        "\n",
        "URL=http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/$FILE.tar.gz\n",
        "TAR_FILE=./datasets/$FILE.tar.gz\n",
        "TARGET_DIR=./datasets/$FILE/\n",
        "\n",
        "\n",
        "if [ -d $TARGET_DIR ]; then\n",
        "    echo $TARGET_DIR exists, not downloading\n",
        "else\n",
        "    mkdir -p $TARGET_DIR\n",
        "    wget -N $URL -O $TAR_FILE\n",
        "    tar -zxvf $TAR_FILE -C ./datasets/\n",
        "    rm $TAR_FILE\n",
        "fi\n",
        "\n"
      ],
      "metadata": {
        "id": "JMHnAibAA5vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ],
      "metadata": {
        "id": "FyDfiYMkyHmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_EXTENSIONS = [\n",
        "    '.jpg', '.jpeg',\n",
        "    '.png', '.ppm', '.bmp',\n",
        "    '.tif', '.tiff',\n",
        "]\n",
        "\n",
        "def is_image_file(filename):\n",
        "    return any(filename.lower().endswith(ext) for ext in IMG_EXTENSIONS)\n",
        "\n",
        "def get_all_image_paths(root):\n",
        "    paths = []\n",
        "    assert os.path.isdir(root)\n",
        "\n",
        "    for root, _, filenames in sorted(os.walk(root)):\n",
        "        for filename in filenames:\n",
        "            if is_image_file(filename):\n",
        "                paths.append(os.path.join(root, filename))\n",
        "    \n",
        "    return paths"
      ],
      "metadata": {
        "id": "emuZ1ZgzFUut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "4FFPAVn8yP8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root):\n",
        "        self.paths = sorted(get_all_image_paths(root))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        A, B = self._split_input_output(self._read_im(self.paths[i]))\n",
        "\n",
        "        transform = self._generate_transform()\n",
        "        A, B = transform(A), transform(B)  # apply same transform to both A and B\n",
        "        \n",
        "        return A, B if sconfig.A_to_B else B, A\n",
        "\n",
        "    def _read_im(self, path):\n",
        "        return Image.open(path).convert('RGB')\n",
        "\n",
        "    def _split_input_output(self, AB):\n",
        "        w, h = AB.size\n",
        "        w2 = int(w / 2)\n",
        "        A = AB.crop((0, 0, w2, h))\n",
        "        B = AB.crop((w2, 0, w, h))\n",
        "\n",
        "        return A, B\n",
        "\n",
        "    def _generate_transform(self):\n",
        "        new_size = 286\n",
        "        old_size = 256\n",
        "\n",
        "        rand_x = random.randint(0, new_size - old_size)\n",
        "        rand_y = random.randint(0, new_size - old_size)\n",
        "        flip = random.random() > 0.5\n",
        "        \n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((new_size, new_size), interpolation=InterpolationMode.BICUBIC, antialias=True),\n",
        "            transforms.Lambda(lambda im: self._crop(im, (rand_x, rand_y), (old_size, old_size))),\n",
        "            transforms.Lambda(lambda im: self._flip(im, flip)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "\n",
        "    def _flip(self, im, flip):\n",
        "        if flip:\n",
        "            return im.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        return im\n",
        "\n",
        "    def _crop(self, im, pos, size):\n",
        "        return im.crop((pos[0], pos[1], pos[0] + size[0], pos[1] + size[1]))"
      ],
      "metadata": {
        "id": "gab1vj-XH3UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criterion"
      ],
      "metadata": {
        "id": "_hHFHrTkC_AN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GANLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, real_label=1.0, fake_label=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.register_buffer('real_label', torch.tensor(real_label))\n",
        "        self.register_buffer('fake_label', torch.tensor(fake_label))\n",
        "\n",
        "        self.loss = nn.BCEWithLogitsLoss() # nn.MSELoss()\n",
        "    \n",
        "    def __call__(self, model_output, target_is_real):\n",
        "\n",
        "        label = self.real_label if target_is_real else self.fake_label\n",
        "        label = label.expand_as(model_output)\n",
        "\n",
        "        return self.loss(model_output, label)"
      ],
      "metadata": {
        "id": "OdhEYBt5DBIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "MZ6AD7e2mN7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ],
      "metadata": {
        "id": "k8DHdOUqp5Hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_time(seconds):\n",
        "    return time.strftime('%Hh:%Mm:%Ss', time.gmtime(seconds))\n",
        "\n",
        "def save_checkpoint(net_G, net_D, optimizer_G, optimizer_D, epoch):\n",
        "    file_name = f'{sconfig.run_id}_epoch_{epoch}.ckpt'\n",
        "    torch.save({\n",
        "        'net_G_state_dict': net_G.state_dict(),\n",
        "        'net_D_state_dict': net_D.state_dict(),\n",
        "        'net_G_optimizer_state_dict': optimizer_G.state_dict(),\n",
        "        'net_D_optimizer_state_dict': optimizer_D.state_dict(),\n",
        "        'session_config': sconfig,\n",
        "        'epoch': epoch\n",
        "    }, file_name)\n",
        "    save_file(file_name, local=False)\n",
        "\n",
        "def load_checkpoint(run_id, epoch):\n",
        "    file_name = f'{run_id}_epoch_{epoch}.ckpt'\n",
        "    load_file(file_name)  # ensure exists locally\n",
        "    checkpoint = torch.load(file_name)\n",
        "\n",
        "    sconfig = checkpoint['session_config']\n",
        "\n",
        "    net_G = Generator(sconfig.generator_config)\n",
        "    net_D = Discriminator(sconfig.discriminator_config)\n",
        "    net_G.load_state_dict(checkpoint['net_G_state_dict'])\n",
        "    net_D.load_state_dict(checkpoint['net_D_state_dict'])\n",
        "\n",
        "    optimizer_G = optim.Adam(net_G.parameters(), lr=sconfig.lr, betas=(sconfig.optimizer_beta1, sconfig.optimizer_beta2))\n",
        "    optimizer_D = optim.Adam(net_D.parameters(), lr=sconfig.lr, betas=(sconfig.optimizer_beta1, sconfig.optimizer_beta2))\n",
        "    optimizer_G.load_state_dict(checkpoint['net_G_optimizer_state_dict'])\n",
        "    optimizer_D.load_state_dict(checkpoint['net_D_optimizer_state_dict'])\n",
        "\n",
        "    return sconfig, net_G, net_D, optimizer_G, optimizer_D, checkpoint['epoch']\n",
        "\n",
        "def set_requires_grad(net, requires_grad):\n",
        "    for param in net.parameters():\n",
        "        param.requires_grad = requires_grad\n"
      ],
      "metadata": {
        "id": "nQziH1Lir5tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_batch(net_G, net_D, optimizer_G, optimizer_D, real_A, real_B, criterion_gan, criterion_l1):\n",
        "\n",
        "    # forward pass\n",
        "    # generate fake image using generator\n",
        "    fake_B = net_G(real_A)\n",
        "\n",
        "    ###\n",
        "    # discrminator\n",
        "    ###\n",
        "    set_requires_grad(net_D, True)\n",
        "    optimizer_D.zero_grad()\n",
        "\n",
        "    # discrminate fake image\n",
        "    fake_AB = torch.cat((real_A, fake_B), dim=1)  # conditionalGAN takes both real and fake image\n",
        "    pred_fake = net_D(fake_AB.detach())\n",
        "    loss_D_fake = criterion_gan(pred_fake, False)\n",
        "\n",
        "    # discrminate real image\n",
        "    real_AB = torch.cat((real_A, real_B), dim=1)\n",
        "    pred_real = net_D(real_AB)\n",
        "    loss_D_real = criterion_gan(pred_real, True)\n",
        "\n",
        "    # update\n",
        "    loss_D = (loss_D_fake + loss_D_real) * sconfig.d_loss_factor\n",
        "    loss_D.backward()\n",
        "    optimizer_D.step()\n",
        "\n",
        "    ###\n",
        "    # generator\n",
        "    ###\n",
        "    set_requires_grad(net_D, False)\n",
        "    optimizer_G.zero_grad()\n",
        "\n",
        "    # generator should fool the discriminator\n",
        "    fake_AB = torch.cat((real_A, fake_B), dim=1)\n",
        "    pred_fake = net_D(fake_AB)\n",
        "    loss_G_fake = criterion_gan(pred_fake, True)\n",
        "\n",
        "    # l1 loss between generated and real image for more accurate output\n",
        "    loss_G_l1 = criterion_l1(fake_B, real_B) * sconfig.l1_lambda\n",
        "\n",
        "    # update\n",
        "    loss_G = loss_G_fake + loss_G_l1\n",
        "    loss_G.backward()\n",
        "    optimizer_G.step()\n",
        "\n",
        "    return loss_G_fake.item(), loss_G_l1.item(), loss_D.item()"
      ],
      "metadata": {
        "id": "YsBcPbyyyMmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lambda_rule(epoch):\n",
        "    return 1.0 - max(0, epoch + sconfig.start_epoch - sconfig.end_epoch) / float(sconfig.epochs_decay + 1)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def init_weight(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if hasattr(m, 'weight') and (classname.find('Conv') != -1 \\\n",
        "                                or classname.find('Linear') != -1 \\\n",
        "                                or classname.find('BatchNorm2d') != -1):\n",
        "        nn.init.normal_(m.weight.data, 0.0, sconfig.init_gain)\n",
        "        if hasattr(m, 'bias') and m.bias is not None:\n",
        "                nn.init.constant_(m.bias.data, 0.0)"
      ],
      "metadata": {
        "id": "cmfHirJYOl1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loop"
      ],
      "metadata": {
        "id": "Zb72gex4CCIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MyDataset('./datasets/facades/train')\n",
        "dataloader = DataLoader(dataset, batch_size=sconfig.batch_size, shuffle=sconfig.shuffle, num_workers=sconfig.num_workers, pin_memory=sconfig.pin_memory)\n",
        "\n",
        "net_G = Generator(generator_config)\n",
        "net_D = Discriminator(discriminator_config)\n",
        "\n",
        "net_G.apply(init_weight)\n",
        "net_D.apply(init_weight)\n",
        "\n",
        "criterion_l1 = nn.L1Loss()\n",
        "criterion_gan = GANLoss().to(device)\n",
        "\n",
        "optimizer_G = optim.Adam(net_G.parameters(), lr=sconfig.lr, betas=(sconfig.optimizer_beta1, sconfig.optimizer_beta2))\n",
        "optimizer_D = optim.Adam(net_D.parameters(), lr=sconfig.lr, betas=(sconfig.optimizer_beta1, sconfig.optimizer_beta2))\n",
        "\n",
        "scheduler_G = optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambda_rule)\n",
        "scheduler_D = optim.lr_scheduler.LambdaLR(optimizer_D, lr_lambda=lambda_rule)\n",
        "\n",
        "net_G.to(device)\n",
        "net_D.to(device)\n",
        "\n",
        "training_start_time = time.time()\n",
        "print(f'Training started at {format_time(training_start_time)}')\n",
        "for epoch in range(sconfig.start_epoch, sconfig.end_epoch + 1 + sconfig.epochs_decay):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    epoch_train_G_gan_losses = []\n",
        "    epoch_train_G_l1_losses = []\n",
        "    epoch_train_D_losses = []\n",
        "    for i, (real_A, real_B) in enumerate(dataloader):\n",
        "        net_G.train()\n",
        "        net_D.train()\n",
        "        real_A = real_A.to(device)\n",
        "        real_B = real_B.to(device)\n",
        "\n",
        "        loss_G_gan, loss_G_l1, loss_D = train_batch(net_G, net_D, optimizer_G, optimizer_D, real_A, real_B, criterion_gan, criterion_l1)\n",
        "\n",
        "        epoch_train_G_gan_losses.append(loss_G_gan)\n",
        "        epoch_train_G_l1_losses.append(loss_G_l1)\n",
        "        epoch_train_D_losses.append(loss_D)\n",
        "    \n",
        "    scheduler_G.step()\n",
        "    scheduler_D.step()\n",
        "    \n",
        "    if epoch % sconfig.eval_freq == 0 or epoch == sconfig.start_epoch:\n",
        "        net_G.eval()\n",
        "        net_D.eval()\n",
        "        epoch_eval_loss = None\n",
        "    else:\n",
        "        epoch_eval_loss = None\n",
        "    \n",
        "    if epoch % sconfig.log_freq == 0 or epoch == sconfig.start_epoch:\n",
        "        print(\n",
        "            f'[Epoch={epoch}] ' +\n",
        "            f'[LR={get_lr(optimizer_G):.6f}] ' +\n",
        "            f'[TrainLossGl1={np.mean(epoch_train_G_l1_losses):.4f}] ' +\n",
        "            f'[TrainLossGgan={np.mean(epoch_train_G_gan_losses):.4f}] ' +\n",
        "            f'[TrainLossD={np.mean(epoch_train_D_losses):.4f}] ' +\n",
        "            f'[EpochTime={format_time(time.time() - epoch_start_time)}] ' +\n",
        "            f'[TrainTime={format_time(time.time() - training_start_time)}]' +\n",
        "            (f'[EvalLoss={epoch_eval_loss:.4f}]' if epoch_eval_loss is not None else '')\n",
        "        )\n",
        "\n",
        "    if epoch % sconfig.save_freq == 0 or epoch == sconfig.start_epoch:\n",
        "        save_checkpoint(net_G, net_D, optimizer_G, optimizer_D, epoch)"
      ],
      "metadata": {
        "id": "0oNbNptimW2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "IX1osH6sObRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_im(tensor):\n",
        "    plt.imshow(tensor.squeeze().cpu().detach().numpy().transpose(1,2,0))\n",
        "\n",
        "test_dataset = MyDataset('./datasets/facades/test')\n",
        "\n",
        "inp, tar = random.choice(dataset)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plot_im(inp)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plot_im(tar)\n",
        "\n",
        "net_G.eval()\n",
        "fake = net_G(inp.unsqueeze(0).to(device))\n",
        "plt.subplot(1, 3, 3)\n",
        "plot_im(fake)\n"
      ],
      "metadata": {
        "id": "qiLDu4NrPOUi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}